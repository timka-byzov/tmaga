{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5494153",
   "metadata": {},
   "source": [
    "# Задание в команду Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "079c0e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# я запускался на ШАДовксом кластере, где trl умеет параллелить вычисления\n",
    "# %env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "# %env CUDA_VISIBLE_DEVICES=6,7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcd18556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !nvidia-smi -g 0 -c 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "017eb58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "938d6b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "def clear_device():    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "165ab430",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374634d1",
   "metadata": {},
   "source": [
    "Загрузка обученных моделей. Веса занимают8Gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b83d5aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: yadisk in /home/byzovti/.conda/envs/tmaga0/lib/python3.13/site-packages (3.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Directory 'models/' already exists. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "%pip install yadisk\n",
    "\n",
    "import os\n",
    "import yadisk\n",
    "import zipfile\n",
    "\n",
    "MODELS_DIR = \"models/\"\n",
    "\n",
    "def unzip_file(zip_path, extract_to):\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "    print(f\"Extracted to: {extract_to}\")\n",
    "\n",
    "def download_extract_models():\n",
    "    if os.path.exists(MODELS_DIR):\n",
    "        print(f\"Directory '{MODELS_DIR}' already exists. Skipping download.\")\n",
    "        return\n",
    "\n",
    "    public_url = \"https://disk.yandex.ru/d/KfiH0zJCpratHQ\"\n",
    "    y = yadisk.YaDisk()\n",
    "    \n",
    "    meta = y.get_public_meta(public_url)\n",
    "    zip_name = meta['name']\n",
    "\n",
    "    print(f\"Downloading {zip_name}...\")\n",
    "    y.download_public(public_url, zip_name)\n",
    "\n",
    "    unzip_file(zip_name, MODELS_DIR)\n",
    "\n",
    "    os.remove(zip_name)\n",
    "    print(f\"Removed archive: {zip_name}\")\n",
    "\n",
    "download_extract_models()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2644a093",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class RLHFSettings:\n",
    "    SFT_MODEL_NAME = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
    "    MAX_LENGTH = 512\n",
    "\n",
    "@dataclass\n",
    "class RewardSettings:\n",
    "    EPOCHS = 1\n",
    "    LR = 5e-5\n",
    "    TRAIN_BATCH_SIZE = 64\n",
    "    EVAL_BATCH_SIZE = 64\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ReinforceSettings:\n",
    "    EVAL_BATCH_SIZE = 32\n",
    "    TRAIN_BATCH_SIZE = 4\n",
    "    LR = 5e-5\n",
    "    EPOCHS = 1\n",
    "    GRADIENT_ACCUMULATION_STEPS = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "421ccef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def save_metrics(metrics: dict, filename: str):\n",
    "\n",
    "    metrics_path = Path(\"./metrics\")    \n",
    "    metrics_path.mkdir(parents=True, exist_ok=True)\n",
    "    file_path = metrics_path / filename\n",
    "\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(metrics, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    print(f\"Метрики сохранены в файл: {file_path}\")\n",
    "\n",
    "def show_metrics(filename: str):\n",
    "    file_path = Path(\"./metrics\") / filename\n",
    "    if not file_path.exists():\n",
    "        print(f\"Файл {file_path} не найден.\")\n",
    "        return\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        metrics = json.load(f)\n",
    "\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3e2ce30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "\n",
    "def get_dataset(\n",
    "    test_size=0.2,\n",
    "    train_subset_sizes: int | None = None,\n",
    "    val_subset_size: int | None = None,\n",
    ") -> DatasetDict:\n",
    "    dataset = load_dataset(\"esfrankel17/original_HelpSteer2_binarized\")[\n",
    "        \"average_rating\"\n",
    "    ]\n",
    "    dataset = dataset.train_test_split(test_size=test_size, seed=42)\n",
    "\n",
    "    if train_subset_sizes is not None:\n",
    "        dataset[\"train\"] = dataset[\"train\"].select(range(train_subset_sizes))\n",
    "    if val_subset_size is not None:\n",
    "        dataset[\"test\"] = dataset[\"test\"].select(range(val_subset_size))\n",
    "\n",
    "    return dataset\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def get_tokenizer(tokenizer_path: str):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, padding_side=\"left\")\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    return tokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3fdf1bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(data):\n",
    "    return \"\\n\".join(\n",
    "        [f\"{item['role'].capitalize()}: {item['content']}\" for item in data]\n",
    "    )\n",
    "\n",
    "def format_reward_dataset(example, tokenizer, max_len=512):\n",
    "    prompt_chosen = extract_text(example[\"chosen\"])\n",
    "    prompt_rejected = extract_text(example[\"rejected\"])\n",
    "\n",
    "    kwargs = {\n",
    "        \"padding\": \"max_length\",\n",
    "        \"truncation\": True,\n",
    "        \"max_length\": max_len,\n",
    "        \"return_tensors\": \"pt\",\n",
    "    }\n",
    "    tokens_chosen = tokenizer.encode_plus(prompt_chosen, **kwargs)\n",
    "    tokens_rejected = tokenizer.encode_plus(prompt_rejected, **kwargs)\n",
    "\n",
    "    return {\n",
    "        \"input_ids_chosen\": tokens_chosen[\"input_ids\"][0],\n",
    "        \"attention_mask_chosen\": tokens_chosen[\"attention_mask\"][0],\n",
    "        \"input_ids_rejected\": tokens_rejected[\"input_ids\"][0],\n",
    "        \"attention_mask_rejected\": tokens_rejected[\"attention_mask\"][0],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85edbe21",
   "metadata": {},
   "source": [
    "## Reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b69257e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import RewardTrainer, RewardConfig\n",
    "\n",
    "def get_default_reward_trainer(output_model_dir, model, tokenizer, train_dataset, eval_dataset):\n",
    "    training_args = RewardConfig(\n",
    "        output_dir=output_model_dir,\n",
    "        per_device_train_batch_size=RewardSettings.TRAIN_BATCH_SIZE,\n",
    "        per_device_eval_batch_size=RewardSettings.EVAL_BATCH_SIZE,\n",
    "        num_train_epochs=RewardSettings.EPOCHS,\n",
    "        learning_rate=RewardSettings.LR,\n",
    "        gradient_accumulation_steps=1,\n",
    "        fp16=True,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=200,\n",
    "        logging_steps=25,\n",
    "        max_length=RLHFSettings.MAX_LENGTH,\n",
    "        remove_unused_columns=True,\n",
    "        report_to=\"none\",\n",
    "        gradient_checkpointing=True,\n",
    "        dataloader_num_workers=4,\n",
    "    )\n",
    "\n",
    "    trainer = RewardTrainer(\n",
    "        model=model,\n",
    "        processing_class=tokenizer,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "    )\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cadec4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "def train_reward_model(output_model_dir, get_trainer_func: callable, device=\"cuda\", num_reward_labels=1):\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        RLHFSettings.SFT_MODEL_NAME, num_labels=num_reward_labels\n",
    "    ).to(device)\n",
    "\n",
    "    tokenizer = get_tokenizer(RLHFSettings.SFT_MODEL_NAME)\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    dataset = get_dataset()\n",
    "    dataset = dataset.map(lambda ex: format_reward_dataset(ex, tokenizer))\n",
    "    train_dataset = dataset[\"train\"]\n",
    "    eval_dataset = dataset[\"test\"]\n",
    "\n",
    "    trainer = get_trainer_func(\n",
    "        output_model_dir, model, tokenizer, train_dataset, eval_dataset\n",
    "    )\n",
    "\n",
    "    metrics = {}\n",
    "    metrics[\"default\"] = trainer.evaluate()\n",
    "    print(\"pretrained metrics\", metrics[\"default\"])\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.save_model(output_model_dir)\n",
    "\n",
    "    metrics[\"final\"] = trainer.evaluate()\n",
    "\n",
    "    return metrics, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9233332",
   "metadata": {},
   "source": [
    "Так как подхода к получению reward-оценок планируется 2, давайте напишем обертку для агрегации выходов\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c655a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class RewardModelWrapper(ABC):\n",
    "    def __init__(self, model, tokenizer, device: str = 'cpu'):\n",
    "            \n",
    "        self.model = model.to(device)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.model.eval()\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def _get_logits(self, texts: list[str]) -> torch.Tensor:\n",
    "        inputs = self.tokenizer(\n",
    "            texts, \n",
    "            return_tensors=\"pt\", \n",
    "            padding=True, \n",
    "            truncation=True, \n",
    "            max_length=512\n",
    "        ).to(self.device)\n",
    "        \n",
    "        return self.model(**inputs).logits\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_rewards(self, texts: list[str]) -> torch.Tensor:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff8cae6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScoreRewardModelWrapper(RewardModelWrapper):\n",
    "    def get_rewards(self, texts):\n",
    "        logits = self._get_logits(texts)\n",
    "        return logits.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "809c25f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device=torch.device(\"cuda\")\n",
    "# metrics, reward_model = train_reward_model(\"./models/lv1/reward/model1\", get_default_reward_trainer, device)\n",
    "\n",
    "# save_metrics(metrics, \"reward_default.json\")\n",
    "# metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6225256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default: {'eval_loss': 0.806454062461853, 'eval_accuracy': 0.5171299288946348, 'eval_runtime': 45.7874, 'eval_samples_per_second': 37.914, 'eval_steps_per_second': 0.306}\n",
      "final: {'eval_loss': 0.694042980670929, 'eval_accuracy': 0.5843568196509373, 'eval_runtime': 45.8396, 'eval_samples_per_second': 37.871, 'eval_steps_per_second': 0.305, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "show_metrics(\"reward_default.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f3833d",
   "metadata": {},
   "source": [
    "Accuracy вырос с 0.51 до 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795f036a",
   "metadata": {},
   "source": [
    "sanity check reward модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2213120",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanity_check_reward(reward_wrapper: RewardModelWrapper) -> bool:\n",
    "    dataset = get_dataset(train_subset_sizes=1, val_subset_size=1)\n",
    "    good_text = extract_text(dataset[\"train\"][0][\"chosen\"])\n",
    "    bad_text = extract_text(dataset[\"train\"][0][\"rejected\"])\n",
    "\n",
    "    rewards = reward_wrapper.get_rewards([good_text, bad_text])\n",
    "    good_reward, bad_reward = rewards[0].item(), rewards[1].item()\n",
    "\n",
    "    # print(f\"Текст (хороший): '{good_text}'\")\n",
    "    # print(f\"Оценка: {good_reward:.4f}\\n\")\n",
    "    # print(f\"Текст (плохой): '{bad_text}'\")\n",
    "    # print(f\"Оценка: {bad_reward:.4f}\\n\")\n",
    "\n",
    "    if good_reward > bad_reward:\n",
    "        print(\"✅ Sanity check пройден: 'хороший' ответ получил более высокую оценку.\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"❌ Sanity check не пройден: 'плохой' ответ получил оценку выше или равную 'хорошему'.\")\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "50f4f92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Sanity check пройден: 'хороший' ответ получил более высокую оценку.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "assert sanity_check_reward(\n",
    "    ScoreRewardModelWrapper(\n",
    "        model=AutoModelForSequenceClassification.from_pretrained(\n",
    "            \"./models/lv1/reward/model1\"\n",
    "        ),\n",
    "        tokenizer=get_tokenizer(RLHFSettings.SFT_MODEL_NAME),\n",
    "        device=\"cpu\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975a598e",
   "metadata": {},
   "source": [
    "## Reinforce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c0b362c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "def evaluate(\n",
    "    model,\n",
    "    dataloader,\n",
    "    tokenizer,\n",
    "    reward_wrapper,\n",
    "    device: torch.device,\n",
    ") -> tuple[np.floating, list[float]]:\n",
    "\n",
    "    model.eval()\n",
    "    all_rewards = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluation\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            generated = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_new_tokens=RLHFSettings.MAX_LENGTH,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "            )\n",
    "            decode_generated = tokenizer.batch_decode(generated, skip_special_tokens=True)\n",
    "            rewards = reward_wrapper.get_rewards(decode_generated)\n",
    "            all_rewards.extend(rewards.cpu().numpy().tolist())\n",
    "    return np.mean(all_rewards), all_rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "09c686d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_reinforce_dataset(example, tokenizer):\n",
    "\n",
    "    def extract_prompt(conversation):\n",
    "        return (\n",
    "            \"\\n\".join(\n",
    "                [\n",
    "                    f\"{turn['role'].capitalize()}: {turn['content']}\"\n",
    "                    for turn in conversation[:-1]\n",
    "                ]\n",
    "            )\n",
    "            + \"\\nAssistant: \"\n",
    "        )\n",
    "\n",
    "    tokens = tokenizer(\n",
    "        extract_prompt(example[\"chosen\"]),\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=RLHFSettings.MAX_LENGTH,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    return {k: v[0] for k, v in tokens.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a2e400",
   "metadata": {},
   "source": [
    "Нам нужно подглядывать за графиками обучения по лоссу и реварду модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5a87dcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def plot_metrics(metrics, save_path=None):\n",
    "    clear_output(wait=True)\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    ax1, ax2 = axes\n",
    "\n",
    "    # --- График Loss ---\n",
    "    ax1.plot(metrics[\"steps\"], metrics[\"losses\"], color=\"tab:red\")\n",
    "    ax1.set_title(\"Train Loss\")\n",
    "    ax1.set_xlabel(\"Шаги обучения\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.grid(True)\n",
    "\n",
    "    # --- График Reward ---\n",
    "    ax2.plot(metrics[\"steps\"], metrics[\"rewards\"], color=\"tab:blue\")\n",
    "    ax2.set_title(\"Reward\")\n",
    "    ax2.set_xlabel(\"Шаги обучения\")\n",
    "    ax2.set_ylabel(\"Mean Reward\")\n",
    "    ax2.grid(True)\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"График сохранен в: {save_path}\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e81511a",
   "metadata": {},
   "source": [
    "Имплементируем reinforce обучение с алгоритмом REINFORCE with baseline. В видеокарту (A4000 16GB) на обучении влезает батч размера 4, и, чтобы добиться сходимости, я добавил gradien accumaulation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f12fdb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoModelForSequenceClassification\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def train_reinforce(\n",
    "        reward_wrapper: RewardModelWrapper, \n",
    "        out_model_dir: str, \n",
    "        plot_save_path:str, \n",
    "        device=\"cuda\"\n",
    "    ):\n",
    "    tokenizer = get_tokenizer(RLHFSettings.SFT_MODEL_NAME)\n",
    "    model = AutoModelForCausalLM.from_pretrained(RLHFSettings.SFT_MODEL_NAME).to(device)\n",
    "\n",
    "    dataset = get_dataset(test_size=0.2, train_subset_sizes=1000, val_subset_size=300)\n",
    "    train_dataset = dataset[\"train\"].map(\n",
    "        lambda ex: format_reinforce_dataset(ex, tokenizer)\n",
    "    )\n",
    "    val_dataset = dataset[\"test\"].map(\n",
    "        lambda ex: format_reinforce_dataset(ex, tokenizer)\n",
    "    )\n",
    "    train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "    val_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=ReinforceSettings.TRAIN_BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "    )\n",
    "    val_loader = DataLoader(val_dataset, batch_size=ReinforceSettings.EVAL_BATCH_SIZE)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=ReinforceSettings.LR)\n",
    "\n",
    "    mean_reward_before, _ = evaluate(\n",
    "        model,\n",
    "        val_loader,\n",
    "        tokenizer,\n",
    "        reward_wrapper,\n",
    "        device,\n",
    "    )\n",
    "    print(f\"Initial mean reward: {mean_reward_before:.4f}\")\n",
    "\n",
    "    # --- REINFORCE Algorithm ---\n",
    "    baseline = 0.0\n",
    "    ema_decay = 0.99\n",
    "\n",
    "    for epoch in range(ReinforceSettings.EPOCHS):\n",
    "        print(f\"Training Epoch {epoch + 1}\")\n",
    "        model.train()\n",
    "\n",
    "        metrics = {\n",
    "            \"losses\": [],\n",
    "            \"rewards\": [],\n",
    "            \"steps\": [],\n",
    "        }\n",
    "\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            prompt_len = input_ids.shape[1]\n",
    "\n",
    "            outputs = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_new_tokens=RLHFSettings.MAX_LENGTH,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                return_dict_in_generate=True,\n",
    "                output_scores=True,\n",
    "            )\n",
    "\n",
    "            full_sequences = outputs.sequences\n",
    "            generated_tokens = full_sequences[:, prompt_len:]\n",
    "\n",
    "            rewards = reward_wrapper.get_rewards(tokenizer.batch_decode(generated_tokens, skip_special_tokens=True))\n",
    "\n",
    "            full_attention_mask = (full_sequences != tokenizer.pad_token_id).long()\n",
    "            logits = model(full_sequences, attention_mask=full_attention_mask).logits\n",
    "\n",
    "            log_probs = torch.log_softmax(logits[:, prompt_len - 1 : -1], dim=-1)\n",
    "            selected_log_probs = log_probs.gather(\n",
    "                -1, generated_tokens.unsqueeze(-1)\n",
    "            ).squeeze(-1)\n",
    "\n",
    "            gen_mask = (generated_tokens != tokenizer.pad_token_id).float()\n",
    "            aggregated_log_probs = (selected_log_probs * gen_mask).sum(dim=1)\n",
    "\n",
    "            # EMA\n",
    "            current_mean_reward = rewards.mean().item()\n",
    "            if baseline == 0.0:  # Инициализация на первом шаге\n",
    "                baseline = current_mean_reward\n",
    "            else:\n",
    "                baseline = ema_decay * baseline + (1 - ema_decay) * current_mean_reward\n",
    "\n",
    "            advantages = rewards - baseline\n",
    "            loss = -(advantages.detach() * aggregated_log_probs).mean()\n",
    "            loss = loss / ReinforceSettings.GRADIENT_ACCUMULATION_STEPS\n",
    "            loss.backward()\n",
    "\n",
    "            if (batch_idx + 1) % ReinforceSettings.GRADIENT_ACCUMULATION_STEPS == 0 or (\n",
    "                batch_idx + 1 == len(train_loader)\n",
    "            ):\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                metrics[\"rewards\"].append(current_mean_reward)\n",
    "                metrics[\"losses\"].append(loss.item())\n",
    "                metrics[\"steps\"].append(epoch * len(train_loader) + batch_idx + 1)\n",
    "\n",
    "            if (batch_idx + 1) % ReinforceSettings.GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "                plot_metrics(metrics)\n",
    "            \n",
    "\n",
    "    plot_metrics(metrics, plot_save_path)\n",
    "    mean_reward_after, _ = evaluate(\n",
    "        model, val_loader, tokenizer, reward_wrapper, device\n",
    "    )\n",
    "    print(f\"Default mean reward: {mean_reward_before:.4f}\")\n",
    "    print(f\"Final mean reward: {mean_reward_after:.4f}\")\n",
    "\n",
    "    model.save_pretrained(out_model_dir)\n",
    "    print(\"Model saved to:\", out_model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24eb7a9c",
   "metadata": {},
   "source": [
    "## LEVEL 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8cebea5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear_device()\n",
    "\n",
    "# device = torch.device(\"cuda\")\n",
    "# reward_wrapper = ScoreRewardModelWrapper(\n",
    "#     tokenizer=get_tokenizer(RLHFSettings.SFT_MODEL_NAME),\n",
    "#     model=AutoModelForSequenceClassification.from_pretrained(\n",
    "#         \"./models/lv1/reward/model1\"\n",
    "#     ),\n",
    "#     device=device,\n",
    "# )\n",
    "# train_reinforce(\n",
    "#     reward_wrapper, \"./models/lv1/reinforce/model1\", \"./plots/reinforce1\", device\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d5fe65",
   "metadata": {},
   "source": [
    "![img](./plots/reinforce1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bf7315",
   "metadata": {},
   "source": [
    "Default mean reward: 0.3855\n",
    "Final mean reward: 1.6430"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c0a7c3",
   "metadata": {},
   "source": [
    "Мы имплмементировали метод REINFORCE with baseline и дообучили SFT модель. Видим, что средний reward дообученной модели значительно подрос, по сравнению с reward базовой модели. На графике явно видно, как по мере обучения, происходил рост. Это подтверждает работоспособность метода. Мы добились поставленной в задании цели.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66951a6c",
   "metadata": {},
   "source": [
    "## LEVEL 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "99922837",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_LABELS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8955a25f",
   "metadata": {},
   "source": [
    "### Доработаем Rewaed модель под вероятностную природу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "061617be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRewardTrainer(RewardTrainer):\n",
    "\n",
    "    def __init__(self, *args, loss_function: callable, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.loss_func = loss_function\n",
    "\n",
    "    def compute_loss(\n",
    "        self,\n",
    "        model,\n",
    "        inputs: dict[str, torch.Tensor],\n",
    "        return_outputs: bool = False,\n",
    "        num_items_in_batch=None,\n",
    "    ) -> torch.Tensor | tuple[torch.Tensor, dict[str, torch.Tensor]]:\n",
    "        logits_w = model(\n",
    "            input_ids=inputs[\"input_ids_chosen\"],\n",
    "            attention_mask=inputs[\"attention_mask_chosen\"],\n",
    "            return_dict=True,\n",
    "        ).logits\n",
    "        logits_l = model(\n",
    "            input_ids=inputs[\"input_ids_rejected\"],\n",
    "            attention_mask=inputs[\"attention_mask_rejected\"],\n",
    "            return_dict=True,\n",
    "        ).logits\n",
    "        probs_w = torch.softmax(logits_w, dim=-1)\n",
    "        probs_l = torch.softmax(logits_l, dim=-1)\n",
    "\n",
    "        loss = self.loss_func(\n",
    "            probs_w,\n",
    "            probs_l,\n",
    "        )\n",
    "\n",
    "        if return_outputs:\n",
    "            return loss, {\n",
    "                \"probs_w\": probs_w,\n",
    "                \"probs_l\": probs_l,\n",
    "            }\n",
    "        return loss\n",
    "\n",
    "    def prediction_step(\n",
    "        self,\n",
    "        model,\n",
    "        inputs: dict[str, torch.Tensor],\n",
    "        prediction_loss_only: bool,\n",
    "        ignore_keys: list[str] | None = None,\n",
    "    ) -> tuple[torch.Tensor | None, torch.Tensor | None, torch.Tensor | None]:\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "        if ignore_keys is None:\n",
    "            if hasattr(self.model, \"config\"):\n",
    "                ignore_keys = getattr(\n",
    "                    self.model.config, \"keys_to_ignore_at_inference\", []\n",
    "                )\n",
    "            else:\n",
    "                ignore_keys = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            loss, logits_dict = self.compute_loss(model, inputs, return_outputs=True)\n",
    "\n",
    "        if prediction_loss_only:\n",
    "            return (loss, None, None)\n",
    "\n",
    "        loss = loss.detach()\n",
    "\n",
    "        probs_w = logits_dict[\"probs_w\"]\n",
    "        probs_l = logits_dict[\"probs_l\"]\n",
    "        ratings = torch.arange(\n",
    "            1,\n",
    "            probs_w.size(-1) + 1,\n",
    "            device=probs_w.device,\n",
    "            dtype=probs_w.dtype,\n",
    "        )  # [1, 2, ... num_labels]\n",
    "        expected_chosen = (probs_w * ratings).sum(dim=-1)\n",
    "        expected_rejected = (probs_l * ratings).sum(dim=-1)\n",
    "        logits = torch.stack([expected_chosen, expected_rejected], dim=-1)\n",
    "\n",
    "        labels = torch.zeros(\n",
    "            expected_chosen.size(0), device=expected_chosen.device, dtype=torch.long\n",
    "        )\n",
    "\n",
    "        return loss, logits, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0f1fb8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClfRewardWraper(RewardModelWrapper):\n",
    "    def get_rewards(self, texts):\n",
    "        logits = self._get_logits(texts)\n",
    "        return torch.argmax(logits, dim=-1).float() + 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20b650e",
   "metadata": {},
   "source": [
    "### Функция потерь 1\n",
    "\n",
    "\n",
    "Понятно, что $E[s|x, y] = \\sum_{i=1}^{10} i \\cdot P(s=i | x, y)$\n",
    "\n",
    "Тогда пусть:\n",
    "$r_w = E[s|x, y_w]$ и $r_l = E[s|x, y_l]$\n",
    "\n",
    "И мы можем свести выражение к обчному Reward лоссу (Bradley-Terry модель):\n",
    "$L(x, y_w, y_l) = -E_{(x, y_w, y_l) \\in D}[\\log(\\sigma(r_w - r_l))]$, который мы уже умеем оптимизировать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "174e6f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_prob_loss(probs_w, probs_l):\n",
    "        num_labels = probs_w.shape[-1]\n",
    "        scores_tensor = torch.arange(1, num_labels + 1, device=probs_w.device, dtype=torch.float32)\n",
    "        expected_score_chosen = (probs_w * scores_tensor).sum(dim=-1)\n",
    "        expected_score_rejected = (probs_l * scores_tensor).sum(dim=-1)\n",
    "\n",
    "        loss = -F.logsigmoid(expected_score_chosen - expected_score_rejected).mean()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "df06a3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_naive_prob_trainer(output_model_dir, model, tokenizer, train_dataset, eval_dataset):\n",
    "    training_args = RewardConfig(\n",
    "        output_dir=output_model_dir,\n",
    "        per_device_train_batch_size=RewardSettings.TRAIN_BATCH_SIZE,\n",
    "        per_device_eval_batch_size=RewardSettings.EVAL_BATCH_SIZE,\n",
    "        num_train_epochs=RewardSettings.EPOCHS,\n",
    "        learning_rate=RewardSettings.LR,\n",
    "        fp16=True,\n",
    "        logging_steps=1,\n",
    "        eval_strategy=\"epoch\",\n",
    "        remove_unused_columns=False, # Важно, т.к. мы используем кастомные колонки\n",
    "        report_to=\"none\",\n",
    "        \n",
    "        gradient_checkpointing=True,\n",
    "        dataloader_num_workers=4,\n",
    "    )\n",
    "\n",
    "    trainer = CustomRewardTrainer(\n",
    "        loss_function=naive_prob_loss,\n",
    "        model=model,\n",
    "        processing_class=tokenizer,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "    )\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dec943b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prob_reward_mertics, prob_reward_model = train_reward_model(\"./models/lv2/reward/model1\", get_naive_prob_trainer, num_reward_labels=NUM_LABELS)\n",
    "\n",
    "# save_metrics(prob_reward_mertics, \"reward_naive_prob.json\")\n",
    "# prob_reward_mertics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3f212cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default: {'eval_loss': 0.812110424041748, 'eval_accuracy': 0.48739495798319327, 'eval_runtime': 45.2959, 'eval_samples_per_second': 38.326, 'eval_steps_per_second': 0.309}\n",
      "final: {'eval_loss': 0.6904765367507935, 'eval_accuracy': 0.5623787976729153, 'eval_runtime': 45.9967, 'eval_samples_per_second': 37.742, 'eval_steps_per_second': 0.304, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "show_metrics(\"reward_naive_prob.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "04966d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Sanity check не пройден: 'плохой' ответ получил оценку выше или равную 'хорошему'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sanity_check_reward(\n",
    "    ClfRewardWraper(\n",
    "        tokenizer=get_tokenizer(RLHFSettings.SFT_MODEL_NAME),\n",
    "        model=AutoModelForSequenceClassification.from_pretrained(\n",
    "            \"./models/lv2/reward/model1\"\n",
    "        ),\n",
    "        device=\"cpu\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a2dc5f",
   "metadata": {},
   "source": [
    "Модель не проходит sanity check ...\n",
    "\n",
    "На двух случайных текстах логиты классов reward'a имеют следующие значения.\n",
    "\n",
    "good: [-4.1186,  1.5412,  1.6056, -1.8219,  5.7619,  1.8166, -0.0647,  0.4995, 0.7026, -2.6734]\n",
    "\n",
    "bad: [-4.2168,  1.3883,  1.1636, -2.0543,  5.7150,  3.0130, -0.6192,  1.2283, 0.4289, -2.5724]\n",
    "\n",
    "Видно, что модель не обучилась достаточно, чтобы различить два класса. Хотя думаю проблема здесь не в обучнии, т к в случае good модель больше уверена в reward'e, чем в случае bad, а значит можно догадаться, где good, а где bad. Мы никак не учитываем уверенность модели в классе. Можно было бы учитывать колебания модели путем увеличения количества классов, но по итогу задача сведется к бесконечному их количеству, то бишь к стандартному подходу с одной логитой )\n",
    "Следующей функцией потерь я попробую заставить модель обучиться получше."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b231c6",
   "metadata": {},
   "source": [
    "### Функция потерь 2\n",
    "\n",
    "Давайте пошагово разберем, как выводится и работает функция потерь в вашем `CustomRewardTrainer`.\n",
    "\n",
    "#### 1. Цель\n",
    "\n",
    "Основная цель — максимизировать вероятность того, что случайно выбранная оценка для \"победителя\" ($S_w$) будет строго больше, чем случайно выбранная оценка для \"проигравшего\" ($S_l$).\n",
    "\n",
    "Математически это записывается как: **максимизировать $P(S_w > S_l)$**.\n",
    "\n",
    "#### 2. Вывод формулы\n",
    "\n",
    "Пусть модель для \"победителя\" $y_w$ предсказала распределение вероятностей $P_w = [p_{w,1}, p_{w,2}, ..., p_{w,N}]$, где $p_{w,i} = P(S_w=i)$. Аналогично для \"проигравшего\" $y_l$ есть распределение $P_l = [p_{l,1}, p_{l,2}, ..., p_{l,N}]$.\n",
    "\n",
    "По закону полной вероятности, $P(S_w > S_l)$ — это сумма вероятностей всех несовместных событий, где оценка \"победителя\" $i$ больше оценки \"проигравшего\" $j$.\n",
    "\n",
    "$P(S_w > S_l) = \\sum_{i=2}^{N} \\sum_{j=1}^{i-1} P(S_w=i \\text{ и } S_l=j)$\n",
    "\n",
    "Поскольку модель оценивает $y_w$ и $y_l$ независимо, мы можем записать:\n",
    "\n",
    "$P(S_w=i \\text{ и } S_l=j) = P(S_w=i) \\cdot P(S_l=j) = p_{w,i} \\cdot p_{l,j}$\n",
    "\n",
    "Итоговая формула:\n",
    "\n",
    "$P(S_w > S_l) = \\sum_{i=2}^{N} \\sum_{j=1}^{i-1} p_{w,i} \\cdot p_{l,j}$, что на практике конечно удобно считать через $\\Sigma \\log(P)$\n",
    "\n",
    "$L(x, y_w, y_l) = -E_{(x, y_w, y_l) \\in D} \\left[ \\log \\left( \\sum_{i=2}^{N} \\sum_{j=1}^{i-1} P(S=i|x, y_w) \\cdot P(S=j|x, y_l) \\right) \\right]$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bafac9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def super_prob_loss(probs_w, probs_l):\n",
    "        batch_size, num_classes = probs_w.shape\n",
    "        mask = torch.tril(\n",
    "            torch.ones(num_classes, num_classes, device=probs_w.device),\n",
    "            diagonal=-1,\n",
    "        )\n",
    "        prod = probs_w.unsqueeze(2) * probs_l.unsqueeze(1)  # prod[., i, j] = chosen[., i] * rejected[., j]\n",
    "        total_probability = (prod * mask).sum(dim=(1, 2))\n",
    "\n",
    "        loss = -torch.log(total_probability.clamp(min=1e-8)).mean()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb39b26e",
   "metadata": {},
   "source": [
    "#### 3. Как это реализовано\n",
    "Вычислять двойную сумму в цикле неэффективно. Код использует гениальный трюк с матричными операциями для вычисления этой суммы для всего батча сразу.\n",
    "\n",
    "1.  **`probs_w` и `probs_l`**\n",
    "    Это наши распределения $P_w$ и $P_l$ для каждого примера в батче.\n",
    "    Форма: `[batch_size, num_classes]`\n",
    "\n",
    "2.  **`prod = probs_w.unsqueeze(2) * probs_l.unsqueeze(1)`**\n",
    "    Здесь создается матрица попарных произведений вероятностей.\n",
    "    *   `probs_w.unsqueeze(2)` имеет форму `[B, N, 1]`.\n",
    "    *   `probs_l.unsqueeze(1)` имеет форму `[B, 1, N]`.\n",
    "    *   Их произведение `prod` имеет форму `[B, N, N]`. Элемент `prod[b, i, j]` для примера `b` в батче равен в точности $p_{w,i} \\cdot p_{l,j}$.\n",
    "\n",
    "3.  **`mask = torch.tril(..., diagonal=-1)`**\n",
    "    `torch.tril` создает нижнетреугольную матрицу. Параметр `diagonal=-1` означает, что главная диагональ и все что выше нее будет заполнено нулями.\n",
    "    ```\n",
    "    [[0, 0, 0],\n",
    "     [1, 0, 0],  // i=2, j=1\n",
    "     [1, 1, 0]]  // i=3, j=1; i=3, j=2\n",
    "    ```\n",
    "    Эта маска идеально соответствует условию нашей суммы: она равна 1 там, где `i > j`, и 0 в остальных случаях.\n",
    "\n",
    "4.  **`total_probability = (prod * mask).sum(dim=(1, 2))`**\n",
    "    *   `prod * mask`: Поэлементное умножение обнуляет все вероятности, где `i <= j`.\n",
    "    *   `.sum(dim=(1, 2))`: Суммирование по измерениям `i` и `j` как раз и вычисляет нашу двойную сумму $\\sum_{i} \\sum_{j, j<i} p_{w,i} \\cdot p_{l,j}$ для каждого элемента батча.\n",
    "\n",
    "5.  **`loss = -torch.log(total_probability.clamp(min=1e-8)).mean()`**\n",
    "    \n",
    "    Вместо того чтобы максимизировать вероятность `P`, мы минимизируем ее `-log(P)`. Это более стабильно численно и дает тот же результат. `.mean()` усредняет loss по всему батчу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1f94b446",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_super_prob_trainer(output_model_dir, model, tokenizer, train_dataset, eval_dataset):\n",
    "    training_args = RewardConfig(\n",
    "        output_dir=output_model_dir,\n",
    "        per_device_train_batch_size=RewardSettings.TRAIN_BATCH_SIZE,\n",
    "        per_device_eval_batch_size=RewardSettings.EVAL_BATCH_SIZE,\n",
    "        num_train_epochs=RewardSettings.EPOCHS,\n",
    "        learning_rate=RewardSettings.LR,\n",
    "        fp16=True,\n",
    "        logging_steps=1,\n",
    "        eval_strategy=\"epoch\",\n",
    "        remove_unused_columns=False, # Важно, т.к. мы используем кастомные колонки\n",
    "        report_to=\"none\",\n",
    "        \n",
    "        gradient_checkpointing=True,\n",
    "        dataloader_num_workers=4,\n",
    "    )\n",
    "\n",
    "    trainer = CustomRewardTrainer(\n",
    "        loss_function=super_prob_loss,\n",
    "        model=model,\n",
    "        processing_class=tokenizer,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "    )\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9814d4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sprob_reward_mertics, prob_reward_model = train_reward_model(\"./models/lv2/reward/model2\", get_super_prob_trainer, num_reward_labels=NUM_LABELS)\n",
    "# sprob_reward_mertics\n",
    "\n",
    "# save_metrics(sprob_reward_mertics, \"reward_super_prob.json\")\n",
    "# sprob_reward_mertics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ef7c8341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default: {'eval_loss': 1.114506721496582, 'eval_accuracy': 0.4654169360051713, 'eval_runtime': 46.1227, 'eval_samples_per_second': 37.639, 'eval_steps_per_second': 0.304}\n",
      "final: {'eval_loss': 0.8250414729118347, 'eval_accuracy': 0.5895281189398837, 'eval_runtime': 46.0345, 'eval_samples_per_second': 37.711, 'eval_steps_per_second': 0.304, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "show_metrics(\"reward_super_prob.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4b8c3b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Sanity check не пройден: 'плохой' ответ получил оценку выше или равную 'хорошему'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sanity_check_reward(\n",
    "    ClfRewardWraper(\n",
    "        tokenizer=get_tokenizer(RLHFSettings.SFT_MODEL_NAME),\n",
    "        model=AutoModelForSequenceClassification.from_pretrained(\n",
    "            \"./models/lv2/reward/model2\"\n",
    "        ),\n",
    "        device=\"cpu\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1871b286",
   "metadata": {},
   "source": [
    "Обучиться по лучше особо не вышло, на мой взглед проблема та же, что и в первом случае. Попытки улучшить лосс тут не помогут."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db737bfc",
   "metadata": {},
   "source": [
    "### Reward 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c940dccc",
   "metadata": {},
   "source": [
    "Получить хорошую reward модель не получилось, но, ради интереса, давайте запустим reinfoce на том, что есть - буду использовать модель обученную второй по счету. Может быть в среднем модель будет все таки попадать в правильные классы $(cls_{good} > cls_{bad})$ и средний reward будет хорош."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d307177a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear_device()\n",
    "# device = torch.device(\"cuda\")\n",
    "\n",
    "# reward_wrapper = ClfRewardWraper(\n",
    "#     tokenizer=get_tokenizer(RLHFSettings.SFT_MODEL_NAME),\n",
    "#     model=AutoModelForSequenceClassification.from_pretrained(\n",
    "#         \"./models/lv2/reward/model2\"\n",
    "#     ),\n",
    "#     device=device,\n",
    "# )\n",
    "# train_reinforce(\n",
    "#     reward_wrapper, \"./models/lv2/reinforce/model2\", \"./plots/reinforce_super.png\", device\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7e6335",
   "metadata": {},
   "source": [
    "![img](./plots/reinforce_super.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b6cf36",
   "metadata": {},
   "source": [
    "В целом можно заметить, что средний reward не вырос и колеблется с медианным значением 6, куда чаще всего попадают близкие друг к другу логиты верного и невергого вариантов ответа. Можно сделать вывод, что reward модель неудачная и улучшить alignament нашей SFT модели, при таком подходе к обучениею, не получилось."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tmaga0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
